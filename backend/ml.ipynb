{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stella's Notes: This is the .py file Stella is currently using to write the NLP code for the project.\n",
    "#Need to do:\n",
    "#Incorporate age \n",
    "#How to save cleaned/stemmed/tokenized lyrics as df? How to save the dicts/inverted indexes etc as well?\n",
    "#How to prioritize certain words above others. Should I even do this? E.g if someone enter \"happy Monday\", should I prioritize happy or Monday?\n",
    "#How to semantically relate words? E.g cold with frozen, etc\n",
    "#Handle misspelled words\n",
    "\n",
    "#Create a datasets for: cleaned lyrics, tokenized lyrics, lyric polarized score, other essential stuff.\n",
    "\n",
    "import json\n",
    "import os\n",
    "from flask import Flask, render_template, request\n",
    "from flask_cors import CORS\n",
    "from helpers.MySQLDatabaseHandler import MySQLDatabaseHandler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spotify_df = pd.read_csv(\"mini_spotify_track_db.csv\")\n",
    "lyric_df = pd.read_csv(\"mini_spotify_db.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                track_id                 artists  \\\n",
       "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                          album_name  \\\n",
       "0                                             Comedy   \n",
       "1                                   Ghost (Acoustic)   \n",
       "2                                     To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "4                                            Hold On   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "0                      Comedy          73       230666     False   \n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "\n",
       "   danceability  energy  ...  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.676  0.4610  ...    -6.746     0       0.1430        0.0322   \n",
       "1         0.420  0.1660  ...   -17.235     1       0.0763        0.9240   \n",
       "2         0.438  0.3590  ...    -9.734     1       0.0557        0.2100   \n",
       "3         0.266  0.0596  ...   -18.515     1       0.0363        0.9050   \n",
       "4         0.618  0.4430  ...    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  time_signature  track_genre  \n",
       "0          0.000001    0.3580    0.715   87.917               4     acoustic  \n",
       "1          0.000006    0.1010    0.267   77.489               4     acoustic  \n",
       "2          0.000000    0.1170    0.120   76.332               4     acoustic  \n",
       "3          0.000071    0.1320    0.143  181.740               3     acoustic  \n",
       "4          0.000000    0.0829    0.167  119.949               4     acoustic  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  Unnamed: 4  Unnamed: 5  \\\n",
       "0  Look at her face, it's a wonderful face  \\r\\nA...         NaN         NaN   \n",
       "1  Take it easy with me, please  \\r\\nTouch me gen...         NaN         NaN   \n",
       "2  I'll never know why I had to go  \\r\\nWhy I had...         NaN         NaN   \n",
       "3  Making somebody happy is a question of give an...         NaN         NaN   \n",
       "4  Making somebody happy is a question of give an...         NaN         NaN   \n",
       "\n",
       "   Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  Unnamed: 10  Unnamed: 11  \\\n",
       "0         NaN         NaN         NaN         NaN          NaN          NaN   \n",
       "1         NaN         NaN         NaN         NaN          NaN          NaN   \n",
       "2         NaN         NaN         NaN         NaN          NaN          NaN   \n",
       "3         NaN         NaN         NaN         NaN          NaN          NaN   \n",
       "4         NaN         NaN         NaN         NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 12  Unnamed: 13  Unnamed: 14  Unnamed: 15  \n",
       "0          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Look at her face, it's a wonderful face  \\r\\nA...\n",
       "1      Take it easy with me, please  \\r\\nTouch me gen...\n",
       "2      I'll never know why I had to go  \\r\\nWhy I had...\n",
       "3      Making somebody happy is a question of give an...\n",
       "4      Making somebody happy is a question of give an...\n",
       "                             ...                        \n",
       "530      \\r\\n  \\r\\n  \\r\\nHey Ah Na Na  \\r\\nInnocence ...\n",
       "531    Yeah! Yeah!  \\r\\n  \\r\\nYou left the lights on ...\n",
       "532    When I waken, and I'm achin', time for sleepin...\n",
       "533    What the hell am I?  \\r\\nThousand eyes, a fly ...\n",
       "534    Yeah, I want to travel south this year  \\r\\nAa...\n",
       "Name: text, Length: 535, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from collections.abc import Callable\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything. Do not remove duplicate words.\n",
    "    Requirement: Use Regex to satisfy this function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input string to be tokenized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of strings representing the words in the text.\n",
    "    \"\"\"\n",
    "    outcome = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lyrics = lyric_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_lyrics = {i: row for i, row in enumerate(lyric_df.to_dict(orient='records'))}\n",
    "#A dictionary of dictionaries. \n",
    "#Key: Row number. Value: Dict of {artist:..., link:...., song:..., text:...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics(dict_of_lyrics, tokenize_method: Callable[[str], List[str]]):\n",
    "    tokenized_lyrics = {}\n",
    "    nrows = len(lyric_df)\n",
    "    for i in range(nrows):\n",
    "        lyrics = dict_of_lyrics[i]['text']\n",
    "        tokenized_lyrics[i] = tokenize_method(lyrics)\n",
    "    return tokenized_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics = tokenize_lyrics(dict_of_lyrics, tokenize)\n",
    "#A dict with the row numbers of lyric_df as keys and the lsit of tokenized lyrics as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_word_song_count(tokenize_method: Callable[[str], List[str]],\n",
    "    tokenized_lyrics: Dict[int, List[str]]):\n",
    "    \"\"\"Returns a dictionary with the row numbers of songs each distinct word appears in as VALUEs and distinct words as KEYS\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenize_method : Callable[[str], List[str]]\n",
    "        A method to tokenize a string into a list of strings representing words.\n",
    "    tokenized_lyrics: \n",
    "        A dict with the row numbers of lyric_df as keys and the lsit of tokenized lyrics as value\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, int]\n",
    "        A dictionary of words mapped to the number of songs they appear in.\n",
    "    \"\"\"\n",
    "    song_count = {}\n",
    "    \n",
    "    for key in tokenized_lyrics:\n",
    "        unique_tokens = set(token.casefold() for token in tokenized_lyrics[key])\n",
    "\n",
    "        for token in unique_tokens:\n",
    "            if token not in song_count:\n",
    "                song_count[token] = set()\n",
    "            song_count[token].add(key)\n",
    "\n",
    "    return song_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_count = build_word_song_count(tokenize, tokenized_lyrics)\n",
    "# A dict with KEYS as unique words and VALUES as a list of all the songs(their row numbers) the word appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juanruzhang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(list_of_stop_words,tokenized_lyrics):\n",
    "    \n",
    "    \"\"\"Returns a dictionary with KEYS as row values and VALUES as lyrics without stop words\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_stop_words : A list of stop words(E.g and, so, the....)\n",
    "    tokenized_lyrics: \n",
    "        A dict with the row numbers of lyric_df as KEYS and the list of tokenized lyrics as VALUE\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, int]\n",
    "        A dictionary of row numbers mapped to the cleaned tokenized lyrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_stop_words = set()\n",
    "    for word in list_of_stop_words:\n",
    "        clean_stop_words.update(nltk.word_tokenize(word.lower()))  \n",
    "    \n",
    "    stop_word_removed_lyrics = {}\n",
    "    \n",
    "    for i, lyrics in tokenized_lyrics.items():\n",
    "        cleaned_lyrics = [word for word in lyrics if word.lower() not in clean_stop_words]\n",
    "        stop_word_removed_lyrics[i] = cleaned_lyrics\n",
    "                \n",
    "    return stop_word_removed_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokenized_lyrics = remove_stop_words(stopwords.words('english'),tokenized_lyrics)\n",
    "#A dictionary of row numbers mapped to the cleaned tokenized lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_input(tokenize,list_of_stop_words,input_words):\n",
    "    \n",
    "    \"\"\"Returns a dictionary with KEYS as row values and VALUES as lyrics without stop words\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_stop_words : A list of stop words(E.g and, so, the....)\n",
    "    input_words: A string(The input)\n",
    "    Returns\n",
    "    -------\n",
    "    A LIST of tokenized words with stop words removed\n",
    "    \"\"\"\n",
    "    list_tokens = tokenize(input_words)\n",
    "    cleaned_words = [word for word in list_tokens if word not in list_of_stop_words]\n",
    "    \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_song_count = build_word_song_count(tokenize, cleaned_tokenized_lyrics)\n",
    "# A dict with KEYS as unique NON STOP words and VALUES as a list of all the songs(their row numbers) the word appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Long runtime\n",
    "def create_j_sim_mat(input_dict) -> np.ndarray:\n",
    "    \"\"\"Create Jaccard similarity matrix for songs.\n",
    "    Create Jaccard similarity matrix, a np.ndarray of size (`len(lyric_df)`, `len(lyric_df)`),\n",
    "    computing the character similarity, where the entry (i, j) indicating the Jaccard similarity\n",
    "    between the songs `i` and `j`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dict: input dictionary of KEYS as song row numbers and VALUES as their lyrics\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The Jaccard similarity matrix of (`number of rows`, `number of rows`), with the entry\n",
    "        (i, j) indicating the Jaccard similarity between the songs `i` and `j`.\n",
    "    \"\"\"\n",
    "    return_matrix = np.zeros((len(lyric_df),len(lyric_df)))\n",
    "\n",
    "    for song_index in input_dict:\n",
    "      for other_song_index in input_dict:\n",
    "        song_words = set(input_dict[song_index])\n",
    "        other_song_words = set(input_dict[other_song_index])\n",
    "        union = song_words | other_song_words\n",
    "        intersection = song_words & other_song_words\n",
    "        value = (len(intersection)-1)/(len(union)-1) \n",
    "        return_matrix[song_index][other_song_index] = value\n",
    "    \n",
    "    return return_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_inverted_index(songs: dict) -> dict:\n",
    "    \"\"\"Builds an inverted index from the song lyrics.\n",
    "\n",
    "    Arguments\n",
    "    =========\n",
    "\n",
    "    songs: dict\n",
    "        A dictionary where keys are song numbers and values are lists of tokens (words from the lyrics).\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    inverted_index: dict\n",
    "        An inverted index mapping each term to a sorted list of tuples (song_number, term_frequency):\n",
    "        inverted_index[term] = [(s1, tf1), (s2, tf2), ...]\n",
    "\n",
    "    Example\n",
    "    =======\n",
    "\n",
    "    >> songs = {\n",
    "    ...    1: ['love', 'me', 'do', 'love', 'love'],\n",
    "    ...    2: ['hello', 'goodbye', 'hello']\n",
    "    ... }\n",
    "\n",
    "    >> idx = build_inverted_index(songs)\n",
    "\n",
    "    >> idx['love']\n",
    "    [(1, 3),(2,2)....]\n",
    "\n",
    "    >> idx['hello']\n",
    "    [(2, 2)]\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "\n",
    "    for song_num, tokens in songs.items():\n",
    "        token_counts = {}\n",
    "        \n",
    "        for token in tokens:\n",
    "            token_counts[token] = token_counts.get(token, 0) + 1\n",
    "        \n",
    "        for token, count in token_counts.items():\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append((song_num, count))\n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_idf(inv_idx, n_docs, min_df=0, max_df_ratio=1):\n",
    "    \"\"\"Compute term IDF values from the inverted index.\n",
    "    Words that are too frequent or too infrequent get pruned.\n",
    "\n",
    "    Hint: Make sure to use log base 2.\n",
    "\n",
    "    inv_idx: an inverted index as above\n",
    "\n",
    "    n_docs: int,\n",
    "        The number of songs.\n",
    "\n",
    "    min_df: int,\n",
    "        Minimum number of documents a term must occur in.\n",
    "        Less frequent words get ignored.\n",
    "        Documents that appear min_df number of times should be included.\n",
    "\n",
    "    max_df_ratio: float,\n",
    "        Maximum ratio of documents a term can occur in.\n",
    "        More frequent words get ignored.\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "\n",
    "    idf: dict\n",
    "        For each term, the dict contains the idf value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO-5.1\n",
    "    return_dict = {}\n",
    "    \n",
    "    for word, word_list in inv_idx.items():\n",
    "        word_count = len(word_list)\n",
    "        \n",
    "        if word_count >= min_df and word_count / n_docs <= max_df_ratio:\n",
    "            idf = math.log2(n_docs/(1+word_count))\n",
    "            return_dict[word] = idf\n",
    "                \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = build_inverted_index(cleaned_tokenized_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf = compute_idf(inverted_index, len(lyric_df), min_df=0, max_df_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dis(list_of_words):\n",
    "    \"\"\"How many times each word appears within a given text. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_words: a list of tokenized words\n",
    "    \"\"\"\n",
    "    fd = nltk.FreqDist(list_of_words)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "nd girl makes feel fine ever believe mine kind girl without blue ever leaves g\n",
      "nd girl makes feel fine ever believe mine kind girl without blue ever leaves\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(cleaned_tokenized_lyrics[0])\n",
    "text.concordance(\"mine\", lines=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams: Frequent two-word combinations\n",
    "#Trigrams: Frequent three-word combinations\n",
    "#Quadgrams: Frequent four-word combinations\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(text)\n",
    "result = finder.ngram_fd.most_common(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/juanruzhang/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.457, 'pos': 0.543, 'compound': 0.7134}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"Wow, NLTK is not not not terrible!\")\n",
    "# compound ranges from -1 (most negative) to +1 (most positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_word_sentiments(list_of_words):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        list_of_words (_type_): list of words\n",
    "\n",
    "    Returns:\n",
    "        a dictionary with KEYS as positive, negative, neutral and VALUES of sets of words that belong to each category\n",
    "    \"\"\"\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "\n",
    "    for word in list_of_words:\n",
    "        score = sia.polarity_scores(word)[\"compound\"]\n",
    "        if score > 0:\n",
    "            positive.append(word)\n",
    "        elif score < 0:\n",
    "            negative.append(word)\n",
    "        else:\n",
    "            neutral.append(word)\n",
    "    \n",
    "    return {\"positive\":positive, \"negative\":negative, \"neutral\":neutral}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms_of_word(word):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        word (_type_): word\n",
    "\n",
    "    Returns:\n",
    "        _type_: a set of all the synonyms of the word\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            synonyms.append(i.name().replace(\"_\", \" \").lower())\n",
    "\n",
    "    return set(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in cleaned_tokenized_lyrics:\n",
    "    if \"love you\" in \" \".join(cleaned_tokenized_lyrics[key]):\n",
    "        #print(key)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming words\n",
    "ps = PorterStemmer()\n",
    "ps.stem(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTUAL IMPLEMENTATION START HERE\n",
    "Step 1: Get inputs\n",
    "2: Use genre to search for songs\n",
    "3: Find exact matching words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([])\n",
      "dict_keys([])\n",
      "{'gorg': []}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Find exact matching words \n",
    "# \"genre_input\" is the user inputted genre\n",
    "user_genre_input = \"gorge\"\n",
    "\n",
    "#Tokenize and clean(Remove stop words) user input\n",
    "#clean_genre_input : {0:[Cleaned user input]}\n",
    "clean_genre_input = remove_stop_words(stopwords.words('english'),{0:tokenize(user_genre_input)})\n",
    "\n",
    "#A dict with KEYS as unique NON STOP words !!from the user input!! and VALUES as a list of all the songs(their row numbers) the word appears in\n",
    "possible_songs_dict = {}\n",
    "\n",
    "for word in clean_genre_input[0]:\n",
    "    if clean_song_count.get(word) is not None:\n",
    "        #Clean_song_count: A dict with KEYS as unique NON STOP words and VALUES as a list of all the songs(their row numbers) the word appears in\n",
    "        possible_songs_dict[word] = clean_song_count[word]\n",
    "print(possible_songs_dict.keys())\n",
    "    \n",
    "#Stem the words that can't be found in song lyrics and add them to stemmed_user_input\n",
    "stemmed_user_input = []\n",
    "#Stem the words that can't be found in song lyrics and add them to stemmed_user_input, preserve words that are already found in song lyrics\n",
    "stemmed_user_input_preserve_original = []\n",
    "for word in clean_genre_input[0]:\n",
    "    if possible_songs_dict.get(word) is None:\n",
    "        stemmed_user_input_word = ps.stem(word)\n",
    "        stemmed_user_input.append(stemmed_user_input_word)\n",
    "        stemmed_user_input_preserve_original.append(stemmed_user_input_word)\n",
    "    else:\n",
    "        stemmed_user_input_preserve_original.append(word)\n",
    "\n",
    "#Update the list of possible songs by searching for stemmed words in the songs\n",
    "for word in stemmed_user_input:\n",
    "    if clean_song_count.get(word) is not None:\n",
    "        possible_songs_dict[word] = clean_song_count[word]\n",
    "print(possible_songs_dict.keys())\n",
    "\n",
    "#Get the songs that has the most number of relevant words from user input\n",
    "most_common_songs = []\n",
    "if possible_songs_dict:\n",
    "    # Count occurrences of each song index\n",
    "    song_counts = Counter(song for song_list in possible_songs_dict.values() for song in song_list)\n",
    "    # Find the maximum count\n",
    "    max_number = max(song_counts.values(), default=0)\n",
    "    # Return the song indices that appear the most\n",
    "    most_common_songs = [song for song, count in song_counts.items() if count == max_number]\n",
    "\n",
    "#For words in the user input that don't exist in any songs, find the synonyms\n",
    "# A dictionary with KEYS as user input words that don't exist in any songs, VALUES as a list of their synonyms\n",
    "word_synonym_dict = {}\n",
    "for word in stemmed_user_input_preserve_original:\n",
    "    if possible_songs_dict.get(word) is None:\n",
    "        set_of_synonyms = get_synonyms_of_word(word)\n",
    "        word_synonym_dict[word] = list(set_of_synonyms)\n",
    "print(word_synonym_dict)\n",
    "##NEED TO FIX: Not sure how to incorporate synonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(most_common_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total number(including repeats) of inputted words in song\n",
    "if len(most_common_songs) > 40:\n",
    "    most_common_songs = most_common_songs[:40]  \n",
    "\n",
    "sum_of_words_dict = {}\n",
    "\n",
    "for word in clean_genre_input[0]:\n",
    "    if word not in inverted_index:\n",
    "        continue  \n",
    "\n",
    "    inv_index_list = inverted_index[word]\n",
    "    for song_idx, num_words in inv_index_list:\n",
    "        if song_idx not in most_common_songs:\n",
    "            continue\n",
    "\n",
    "        sum_of_words_dict[song_idx] = sum_of_words_dict.get(song_idx, 0) + num_words\n",
    "\n",
    "top_30_songs = sorted(sum_of_words_dict, key=sum_of_words_dict.get, reverse=True)[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(top_30_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find songs without(or with very little) antonyms\n",
    "def get_antonyms(word):\n",
    "    antonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            if i.antonyms():\n",
    "                antonyms.append(i.name().replace(\"_\", \" \").lower())\n",
    "    return set(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get all antonyms for user input words\n",
    "word_antonyms = {word: get_antonyms(word) for word in clean_genre_input[0]}\n",
    "\n",
    "# for the top_30_songs, find the songs with these antonyms\n",
    "song_antonym_counts = Counter()\n",
    "\n",
    "for word, antonyms in word_antonyms.items():\n",
    "    for antonym in antonyms:\n",
    "        if antonym in clean_song_count:\n",
    "            for song in clean_song_count[antonym]:\n",
    "                if song in top_30_songs: \n",
    "                    song_antonym_counts[song] += 1 \n",
    "\n",
    "# Step 3: Add songs with no antonyms with values of 0\n",
    "for song in top_30_songs:\n",
    "    if song not in song_antonym_counts:\n",
    "        song_antonym_counts[song] = 0\n",
    "\n",
    "# Step 4: Sort songs by least distinct antonyms\n",
    "filtered_songs = sorted(song_antonym_counts.keys(), key=lambda song: song_antonym_counts[song])\n",
    "\n",
    "filtered_songs = filtered_songs[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(filtered_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sort the input_song_list based on the polarity score(pos, neg, neutral, compound(Compound being -1 to 1, with -1 being neg and 1 being pos))\n",
    "def sort_polarity_scores(input_song_list, cleaned_tokenized_lyrics, polarity_type=\"compound\"):\n",
    "    sia = SentimentIntensityAnalyzer() \n",
    "    dict_of_polarity_scores = {}\n",
    "\n",
    "    for song_row_number in input_song_list:\n",
    "        if song_row_number in cleaned_tokenized_lyrics:\n",
    "            dict_of_polarity_scores[song_row_number] = sia.polarity_scores(\" \".join(cleaned_tokenized_lyrics[song_row_number]))\n",
    "\n",
    "    # Sort songs by the specified polarity score in descending order\n",
    "    sorted_songs = sorted(dict_of_polarity_scores, key=lambda x: dict_of_polarity_scores[x][polarity_type], reverse=True)\n",
    "\n",
    "    return sorted_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes too long to run\n",
    "result = sort_polarity_scores(filtered_songs, cleaned_tokenized_lyrics, \"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "song_lyrics = cleaned_tokenized_lyrics.values()\n",
    "\n",
    "joined_song_lyrics = []\n",
    "for lyrics in song_lyrics:\n",
    "    joined_song_lyrics.append(\" \".join(lyrics))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(joined_song_lyrics)\n",
    "\n",
    "svd = TruncatedSVD(n_components=min(tfidf_matrix.shape))\n",
    "svd.fit(tfidf_matrix)\n",
    "explained_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "\n",
    "optimal_num_topics = np.argmax(explained_variance >= 0.90) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaar\n"
     ]
    }
   ],
   "source": [
    "#Which index corresponds to what word\n",
    "print(words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: love know want chorus like let time away say got\n",
      "Topic 2: love heart forever chorus true hold want away christmas close\n",
      "Topic 3: love yeah la girl got goin hey little baby said\n",
      "Topic 4: la girl watch mama oh better betty life said world\n",
      "Topic 5: want know make baby la let got better gonna close\n",
      "Topic 6: yeah la walk away chorus turn come let fly live\n",
      "Topic 7: let girl dance come love yeah baby shine music dancing\n",
      "Topic 8: world turn make friendly game card ends depends hey think\n",
      "Topic 9: girl hey baby walk na away chorus kind sa remember\n",
      "Topic 10: na hey sa ang chorus christmas gonna ko ng happy\n",
      "Topic 11: man want na sa woman ang yeah close live world\n",
      "Topic 12: man na sa ang let got gonna walk gotta la\n",
      "Topic 13: walk christmas away gotta somebody dance want turn say round\n",
      "Topic 14: little baby christmas chorus friendly let world ends card stop\n",
      "Topic 15: hey come wanna gonna girl chorus tell believe darling baby\n",
      "Topic 16: girl chorus want queen dancing gonna like dance river yeah\n",
      "Topic 17: come oh baby remember river turn friendly stop said card\n",
      "Topic 18: chance old gonna hey chorus live yes say wonder anymore\n",
      "Topic 19: alive got hey long night queen ha dance right cause\n",
      "Topic 20: got oh walk alive live world ready life chorus woman\n",
      "Topic 21: gotta hey rock ready alive roll stop like wanna die\n",
      "Topic 22: little hey river come gotta want live walk man queen\n",
      "Topic 23: alive little girl world gimme stop live gotta good suzy\n",
      "Topic 24: river alive want let day forever night walk hey gonna\n",
      "Topic 25: walk tonight gonna stop somebody gimme baby wanna hold feeling\n",
      "Topic 26: nights man better make queen think feel good alive gotta\n",
      "Topic 27: alive tell feeling chance feel girl late say chorus really\n",
      "Topic 28: sorry got know gimme remember heart alive stop hand time\n",
      "Topic 29: somebody river got suzy feeling better world like sorry stop\n",
      "Topic 30: honey gimme fly christmas hold strong know lifetime wanna chance\n",
      "Topic 31: anymore wanna live sorry alive way come christmas oh nights\n",
      "Topic 32: gotta rock roll stop world wonder said strong right crazy\n",
      "Topic 33: round got somebody remember believe said good hard road forever\n",
      "Topic 34: hard suzy alive tell said old heart gimme better sweet\n",
      "Topic 35: suzy like remember stay know oh tonight cause god hang\n",
      "Topic 36: know somebody road woman friends head come stay think good\n",
      "Topic 37: honey road alive ha gone ah world try long really\n",
      "Topic 38: say river line alive things way tell dance live somebody\n",
      "Topic 39: tell ready remember feeling road new taking look day stop\n",
      "Topic 40: like feel baby sweet blue good people lord kind new\n",
      "Topic 41: gotta somebody remember woman sweet mind ring mother lonely read\n",
      "Topic 42: sorry forever ring honey lose rock goin city said eyes\n",
      "Topic 43: suzy said rock say long live roll hand river came\n",
      "Topic 44: stop remember wanna road anymore feel gimme honey things hard\n",
      "Topic 45: ready good oh stay said sweet forever time tonight wait\n",
      "Topic 46: try song sing feel gone said hear night inside people\n",
      "Topic 47: sorry ha ah feeling woman old fly baby home gotta\n",
      "Topic 48: ring road ready hold lovin like round gotta head suzy\n",
      "Topic 49: ring round cause fucking shit long taking world music wait\n",
      "Topic 50: ready sorry eyes sweet woman perfect mother true try gotta\n",
      "Topic 51: woman edge mind ready turn read gimme sun heaven shine\n",
      "Topic 52: world good night stay ring ready mother lay believe happy\n",
      "Topic 53: ready gone die old tell said gonna friend happy new\n",
      "Topic 54: dumb ready suzy boomerang time boom walk hang bang dum\n",
      "Topic 55: sorry dumb road fly late miss night room boomerang way\n",
      "Topic 56: edge ring gimme home good head ooh ha turn heaven\n",
      "Topic 57: people city alive ah heart summer goin hand middle suzy\n",
      "Topic 58: ring sorry honey night lovin chance live end said lay\n",
      "Topic 59: dumb boomerang boom dum ring ready bang turn edge sorry\n",
      "Topic 60: ready gimme home song sing swear hear really music girl\n",
      "Topic 61: sweet ring anymore dumb read really goin close burning time\n",
      "Topic 62: ring rock roll somebody late mind way perfect alive wish\n",
      "Topic 63: lay edge sweet ring wait round ha toys kind talk\n",
      "Topic 64: gimme dumb boomerang comes sun high water make ha honey\n",
      "Topic 65: fly toys god somebody goodbye fucking shit things real ah\n",
      "Topic 66: god late nights wanna shine dance lady taking uh ooh\n",
      "Topic 67: swear stop edge really gonna lovin believe chance night gotta\n",
      "Topic 68: swear god ring say heart gone forever new day honey\n",
      "Topic 69: edge waiting hard lovin shine happy somebody head calling sun\n",
      "Topic 70: free wait god feeling tell high fall soul gimme remember\n",
      "Topic 71: edge said late good fucking dance shit oooh oh dreams\n",
      "Topic 72: baby gimme fernando chance feel christmas pain rich run old\n",
      "Topic 73: clear shame remember head line ha fall water america end\n",
      "Topic 74: shame line hold miss money gave bad sun ready bit\n",
      "Topic 75: lay edge stay people dance cause shame suzy rock think\n",
      "Topic 76: queen shine lord live gimme sorry midnight angels light edge\n",
      "Topic 77: edge heart stay boy heaven shit fucking rock feeling honey\n",
      "Topic 78: tell life lovin strong away forever people really happy doctor\n",
      "Topic 79: edge anymore hard ready stay night things people money wait\n",
      "Topic 80: calling anymore arms hear feeling bad fight angels honey falling\n",
      "Topic 81: lord say blue gimme knowing home night line ready head\n",
      "Topic 82: fernando say goin live bring new fucking really ya movin\n",
      "Topic 83: lord fall feel crazy cold wanna cryin happy blue arms\n",
      "Topic 84: fernando christmas free old stop heart world night perfect mother\n",
      "Topic 85: taking ring oooh time head people wanna gave hard suzy\n",
      "Topic 86: fernando crazy sweet lose fool suzy summer lord friend shame\n",
      "Topic 87: sing think walking really song chiquitita fly ready songs chance\n",
      "Topic 88: feeling sweet better fall remember memories nice old arms people\n",
      "Topic 89: clear america crazy wish doctor price thing toys live tonight\n",
      "Topic 90: fernando sweet doctor gimme hand people forever honey grind home\n",
      "Topic 91: fernando things late lay dumb toys shine time rich fall\n",
      "Topic 92: sun believe alive cross home round mind water inside blues\n",
      "Topic 93: people edge heart ha anymore decide better king shine like\n",
      "Topic 94: wishing crazy ring feeling christmas mamma ha stop read today\n",
      "Topic 95: doctor oooh wanna came gun day queen saw close memories\n",
      "Topic 96: shame doctor head like belong night god time live sing\n",
      "Topic 97: fernando high ring feeling cotton shoes gone lay anymore perfect\n",
      "Topic 98: lovin road shame swear lord knowing stay ya easy far\n",
      "Topic 99: fernando god black break wild falling knowing blue mother natural\n",
      "Topic 100: thought natural memories fly shame burning kisses tonight gave live\n",
      "Topic 101: edge natural today home world play taking hearts happy business\n",
      "Topic 102: toys fall talk heart dreamin ooh shame shine lady waiting\n",
      "Topic 103: lord america clear outta ya tonight thinking country fernando natural\n",
      "Topic 104: doctor boy fernando perfect taking calling kind sorry swear christmas\n",
      "Topic 105: forever doctor natural mamma brought pain feeling eyes late line\n",
      "Topic 106: forever shame key dance friend gave longboat water really brought\n",
      "Topic 107: calling stop god walking blue long nights lady fly angels\n",
      "Topic 108: knowing fernando ah bit leave said lucy coming new like\n",
      "Topic 109: best lonely read oooh came turn instead fall line wishing\n",
      "Topic 110: country way nice goodbye hearts shoes life boy stay laugh\n",
      "Topic 111: edge shame happy belong natural na things clear place anymore\n",
      "Topic 112: kiss blue dance cares trip anymore price think soul believes\n",
      "Topic 113: burning crazy burn people lord mind sun leave doctor believer\n",
      "Topic 114: place belong god girls shine talk fucking pick chance wait\n",
      "Topic 115: burning anymore sunny strong kisses middle sun heart belong rich\n",
      "Topic 116: burn burning shame doctor wishing goodbye lonely clear water leave\n",
      "Topic 117: tells ya crazy hold read like sleep movin yes pain\n",
      "Topic 118: swear oooh lord yak na strong taste instead christmas lovin\n",
      "Topic 119: shame days awhile door ya carolina crazy radio lady waiting\n",
      "Topic 120: rich money natural perfect city wrong known na diamonds darling\n",
      "Topic 121: burning control inside burn seen day wait ya trip goodbye\n",
      "Topic 122: na rich eat blue goin sweet givin business fernando moon\n",
      "Topic 123: goodbye high natural country work trip friends crazy dancing hello\n",
      "Topic 124: mamma shoes table shame inside food honey times mountain light\n",
      "Topic 125: doctor na perfect music crazy apple carolina country close chance\n",
      "Topic 126: goodbye natural lose sweet gone lay emotion lovelight today strong\n",
      "Topic 127: doctor afraid shadow words drifting wanna believer lost key diamonds\n",
      "Topic 128: shame doctor wants country blue fall wild die happy cat\n",
      "Topic 129: faith long seen blood wants key cat longboat bread good\n",
      "Topic 130: natural lovin shine bread faith bye blood na cut swear\n",
      "Topic 131: seen toys rich try talk earth shine act oh chance\n",
      "Topic 132: radio maniacs na queen wishing late place body blue awhile\n",
      "Topic 133: rich eat wait tells na natural heaven low winner knowing\n",
      "Topic 134: sword apple eye watches strong goodbye path free faith happy\n",
      "Topic 135: shine movin stop gun belong mercy dum door die key\n",
      "Topic 136: spiderman world place going lights dog kind taking matter sick\n",
      "Topic 137: read king noel crown pages times uh awhile best mother\n",
      "Topic 138: givin middle end spirit lovelight lose shining near sickman right\n",
      "Topic 139: summer tells american anymore people water white known seen radio\n",
      "Topic 140: read anymore shame look swear lucy small radio girls today\n",
      "Topic 141: surprise crawling music hangin calling tells yesterdays keepin soon na\n",
      "Topic 142: ring lines tiger brought happy givin gimme mother seen hurts\n",
      "Topic 143: mamma run say dum precious givin na girls edge high\n",
      "Topic 144: wants cat wild natural middle close surprise tree afraid water\n",
      "Topic 145: noel body fall cries money pick edge glove angels crazy\n",
      "Topic 146: na chance crawling deep cries mood arms trip chuck dreamworld\n",
      "Topic 147: headlines new hearts shadow pink apple long chiquitita american tree\n",
      "Topic 148: longboat key easy faith wish believes seen music tonight wait\n",
      "Topic 149: ball thought believer movin crawling dumb rubber cow inside cries\n",
      "Topic 150: real toys body na nice lot feels glove summer loves\n",
      "Topic 151: radio maniacs afraid dum toys precious words tonite arms doctor\n",
      "Topic 152: carolina dont doctor tells waterloo mother tennessee knowing gave turn\n",
      "Topic 153: tiger shame bale yellow pick glow yesterdays turn raven midget\n",
      "Topic 154: dancin shaggin radio fever boulevard american doctor maniacs sun ang\n",
      "Topic 155: pink natural cries pandora shame cross goodbye slipping oooh open\n",
      "Topic 156: calling lot saw jeanie middle means late tried disgraceland kind\n",
      "Topic 157: doctor look nice spiderman wait lovin tiger britain excited cow\n",
      "Topic 158: low ya refrigerator natural blood diamonds lose afraid heaven nation\n",
      "Topic 159: boulevard shaggin dancin tells shut dog forever things middle act\n",
      "Topic 160: waterloo doctor stuff small shame cares summer dancing tells seen\n",
      "Topic 161: mercy belong place hell marionette tells headlines cow sickman catch\n",
      "Topic 162: believes sword calling tonite ticket shame excited farm low takes\n",
      "Topic 163: church crawling pretty dum cat sick radio told daybreak vale\n",
      "Topic 164: radio nice maniacs teacher shoes carolina kissed travelin burning hug\n",
      "Topic 165: radio maniacs believes mercy na follow gave blues spiderman hide\n",
      "Topic 166: supernatural youth rich department country believe goodbye strong bale mountain\n",
      "Topic 167: marionette wonderland winter dumb touch cow shake chuck rich edge\n",
      "Topic 168: middle pink thinking low wind burning fallin matter cries burn\n",
      "Topic 169: deep afraid bit oooh andante dumb thinking nuclear glow calling\n",
      "Topic 170: mercy farm waterloo church sick vale heaven mama low spend\n",
      "Topic 171: britain taste oooh crawling fever lot burn crackin mercy longer\n",
      "Topic 172: radio maniacs country lot toys precious hold sure middle bring\n",
      "Topic 173: longboat gulf sickman believer key boulevard apple mexico shaggin money\n",
      "Topic 174: mercy dancin shaggin boulevard pink calling givin dumb ha white\n",
      "Topic 175: radio jeanie maniacs faith mother waterloo cow monkey tonite fool\n",
      "Topic 176: mercy damned cow wish midget faith oooh stars brought table\n",
      "Topic 177: waterloo believer treat givin cares dum war jeanie brother life\n",
      "Topic 178: shaggin chuck boulevard crackin shame dancin headlines sickman high pretty\n",
      "Topic 179: mercy givin britain real disgraceland waterloo headlines used blood teacher\n",
      "Topic 180: jeanie cow dumb givin longboat pink sore moo dreamin sight\n",
      "Topic 181: disgraceland jeanie country pick dancin bale shaggin ticket boulevard class\n",
      "Topic 182: givin mercy disgraceland gave believer fist dont stuff damned goodbye\n",
      "Topic 183: britain yesterdays trip glow marionette hoppin nuclear infected na body\n",
      "Topic 184: waterloo cow carolina mercy cut afraid farm times bring remember\n",
      "Topic 185: waterloo farm teacher hug marionette kissed believer sickman disgraceland summer\n",
      "Topic 186: jeanie midget waterloo damned kiss farm rae chung cowboys dong\n",
      "Topic 187: waterloo damned sore believer watches middle act eye noel sparrow\n",
      "Topic 188: britain damned glove maniacs tells radio body cares sickman church\n",
      "Topic 189: headlines cow pink farm middle nice yesterdays daybreak soon lovin\n",
      "Topic 190: cow farm youth moo shame mexico gulf department farmer ha\n",
      "Topic 191: crawling waterloo damned ticket britain hangin feels shut noel express\n",
      "Topic 192: damned cow country chuck disgraceland shake raven noel supernatural crackin\n",
      "Topic 193: britain damned givin cow farm apple faith tiger believer moment\n",
      "Topic 194: midget shaggin dancin boulevard believer true dumb chung rae carolina\n",
      "Topic 195: church noel vale andante strong america born jeanie clear wait\n",
      "Topic 196: cow trip moo hoppin carolina shaggin dancin boulevard farmer givin\n",
      "Topic 197: britain tonite noel youth department afraid jeanie waterloo raven shame\n",
      "Topic 198: tonite youth key department longboat disgraceland givin tells yesterdays nice\n",
      "Topic 199: damned jeanie headlines tells path apple bush lonely crawling messin\n",
      "Topic 200: cow tells waterloo moo cryin excited believes chiquitita fever shut\n",
      "Topic 201: headlines noel ticket mercy express heartbreak class toys strong cut\n",
      "Topic 202: cow tells yesterdays disgraceland britain givin bobby sword control apple\n",
      "Topic 203: britain givin teacher hug shaggin tonite boulevard kissed dancin white\n",
      "Topic 204: cow tonite bit pink tiger faith noel touch loveland chuck\n",
      "Topic 205: mercy youth department outta crackin cow conquers teacher afraid dum\n",
      "Topic 206: damned youth noel headlines department waterloo sickman pink givin excited\n",
      "Topic 207: tonite pink messin arms goodbye believer place travelin teacher afraid\n",
      "Topic 208: jeanie damned oasis yesterdays church youth cut hide department times\n",
      "Topic 209: tonite disillusion disgraceland believer mistletoe memories bobby disillusions hangin dont\n",
      "Topic 210: teacher excited kissed hug britain raven cow nevermore thinking lovelight\n",
      "Topic 211: dont nights watches boulevard calling chuck disgraceland eye shaggin brought\n",
      "Topic 212: shaggin spiderman boulevard dancin summer maniacs crawling darling radio surprise\n",
      "Topic 213: britain youth department headlines crackin middle midget bit turnaround maniacs\n",
      "Topic 214: noel jeanie britain givin glow sickman oasis nuclear infected fade\n",
      "Topic 215: pink church vale marionette tonite damned waterloo bale brown pick\n",
      "Topic 216: damned tonite midget tiger kiss feels excited crackin daybreak disgraceland\n",
      "Topic 217: waterloo disgraceland pink cut american andante apple oasis fingers aha\n",
      "Topic 218: excited daybreak andante dont strong cecilia fever tiger oooh stinks\n",
      "Topic 219: mercy yesterdays disillusion sickman glow dum middle supernatural disgraceland nuclear\n",
      "Topic 220: damned bale raven pick mr dont radio headlines tiger nevermore\n",
      "Topic 221: excited britain cecilia chuck carolina shake business song believer radio\n",
      "Topic 222: marionette believer surprise excited pink britain ticket damned dont glow\n",
      "Topic 223: disgraceland believer glove tiger church youth department spiderman surprise vale\n",
      "Topic 224: cow mercy yak spaced noel moo sore church kisses shaggin\n",
      "Topic 225: marionette tonite jeanie headlines supernatural oasis midget fever faith disillusion\n",
      "Topic 226: britain notion jeanie cow noel mamma chiquitita lovelight walkin wrong\n",
      "Topic 227: jeanie damned marionette disgraceland tiger pink carolina years glove summer\n",
      "Topic 228: pink cecilia disillusion jeanie yonder feels excited rush supernatural oooh\n",
      "Topic 229: tonite crawling dont mercy spaced teacher attack cecilia doin church\n",
      "Topic 230: crawling bad diamonds excited country jeanie gun loaded apple noel\n",
      "Topic 231: sword sup troop midget lovelight talk headlines shining apple path\n",
      "Topic 232: pink tonite headlines study riverside oasis war lights dont shame\n",
      "Topic 233: tonite farm cares marionette disgraceland tells excited nice mercy brother\n",
      "Topic 234: excited stinks replaced jeanie wind loves crackin things dreamworld cares\n",
      "Topic 235: spiderman crawling treat act country brother crackin slower mercy overview\n",
      "Topic 236: carolina tonite sword crackin givin slipping watch trip far fingers\n",
      "Topic 237: daybreak crawling fever cecilia business natural kisses fight say marionette\n",
      "Topic 238: tonite excited damned farm poison believer everyday sword mamma sittin\n",
      "Topic 239: teacher supernatural kissed excited damned hug awhile stinks noel nice\n",
      "Topic 240: church chuck dont vale disgraceland waterloo messin shake work ball\n",
      "Topic 241: marionette church headlines vale dont conquers sup troop supernatural excited\n",
      "Topic 242: headlines andante crawling damned loveland yonder bread country aha tropical\n",
      "Topic 243: noel givin farm fever spend mamma left lovers matters excuse\n",
      "Topic 244: sword excited church waterloo throw darling faith vale late crawling\n",
      "Topic 245: crawling middle disgraceland church replaced carolina vale told stinks tiger\n",
      "Topic 246: act damned habits riverside study sword taste war fever mean\n",
      "Topic 247: marionette noel hoppin travelin dixie mudda chiquitita trip midget break\n",
      "Topic 248: jeanie headlines givin fever yesterdays daybreak lucy dreamworld toys soul\n",
      "Topic 249: yak seen rich write dreamin wanna ways bee bumble voodoo\n",
      "Topic 250: supernatural tonite lovelight believes yesterdays glove excited dreamin toys shining\n",
      "Topic 251: oasis riverside study lucy jeanie war ticket longer lovelight elevator\n",
      "Topic 252: fever marionette cecilia trying carolina nation confess mountain business refrigerator\n",
      "Topic 253: cecilia mercy marionette mother poison crackin confess loved eat runner\n",
      "Topic 254: excited mexico middle gulf andante jeanie messin carolina headlines church\n",
      "Topic 255: daybreak cecilia mercy afraid uh waited teacher confess longboat tar\n",
      "Topic 256: excited jeanie try teacher dreamin stinks confess mercy sore riverside\n",
      "Topic 257: taste india cecilia lovin cream dont sickman peaches lovers watches\n",
      "Topic 258: supernatural dreamin believer farm noel tells country strong confess bit\n",
      "Topic 259: sore oasis glove stinks lifetime hats supernatural work cowboys nice\n",
      "Topic 260: riverside study daybreak war pink chuck poison kisses marionette shake\n",
      "Topic 261: dont yonder spiderman chuck loveland talk shake nation write died\n",
      "Topic 262: ballerina cinderella nina tiger mercy believer watches weed kind excited\n",
      "Topic 263: stinks country tonite dont sup troop oasis chiquitita yonder loves\n",
      "Topic 264: tiger chuck believer sunny yak andante shown waterloo lovelight shake\n",
      "Topic 265: andante hoppin trip yak sickman excited hurry chuck study riverside\n",
      "Topic 266: toys shut study riverside diamonds sore nice mask thought war\n",
      "Topic 267: stinks andante oo tiger ballerina nina cinderella drifting ways oooh\n",
      "Topic 268: messin marionette feels nation hoppin ticket notion cowboys poison express\n",
      "Topic 269: loveland burn pandora dont messin tropical jeanie disgraceland goin excited\n",
      "Topic 270: fever dreamin cecilia tonite ones destroyed oooh ways train andante\n",
      "Topic 271: chiquitita oasis dreamworld chuck clock toys middle mother monkey nation\n",
      "Topic 272: waterloo whiskey lucy cecilia yak bobby ticket aa taste train\n",
      "Topic 273: daybreak sword messin mercy seen bale waterloo monkey youth learn\n",
      "Topic 274: whiskey aa disillusion ace yesterdays voice confess cowboys reason elevator\n",
      "Topic 275: toys believer aha sickman conquers excited saw lovelight fever crown\n",
      "Topic 276: sword tar line loveland conquers thought tonite chosen pages nice\n",
      "Topic 277: pink cord twas sunny attack burnin believes birthday cares jesus\n",
      "Topic 278: diamonds andante teacher yak clock whiskey crawling kissed hug noel\n",
      "Topic 279: cecilia tiger conquers confess aha mm owe roller notion new\n",
      "Topic 280: sickman dreamin loveland crawling sombrero cut disillusion table tropical white\n",
      "Topic 281: daybreak billie stinks yak burn tiger lot millie stars loves\n",
      "Topic 282: andante nights sickman giddy country midnight sad sleigh crackin spiderman\n",
      "Topic 283: tiger business pink cryin believes america feeling door ways seasons\n",
      "Topic 284: daybreak billie waited nation seen messin edge hoppin dreamworld chuck\n",
      "Topic 285: stinks spaced lines angels glove believer headlines beneath outta past\n",
      "Topic 286: andante sickman cecilia afraid automatic clock touch talk money mood\n",
      "Topic 287: feels conquers glove life body replaced clock american bad strong\n",
      "Topic 288: troop sup band roller rikky rocket scream decide pain givin\n",
      "Topic 289: yonder roller glove cecilia rikky excited hill cares sickman pandora\n",
      "Topic 290: conquers andante automatic bobby sleep country spaced ya rise cares\n",
      "Topic 291: fever country oasis messin whiskey mercy pink loveland farm longboat\n",
      "Topic 292: feels lovelight poison oasis messin yonder sword glove realized straight\n",
      "Topic 293: fever chiquitita city disillusion feels spaced country diamonds sickman winner\n",
      "Topic 294: disillusion yonder honolulu hawaii voodoo sore stinks bad sight sickman\n",
      "Topic 295: daybreak farm fever andante sore party calling tells roller dad\n",
      "Topic 296: nation watches merry table cream keepin peaches sparrow voodoo sunshine\n",
      "Topic 297: sunny chiquitita glove bobby farm happiest days came talking cat\n",
      "Topic 298: dreamin voodoo adore daybreak troop sup longer andante nation middle\n",
      "Topic 299: voodoo country wont confess tiger chiquitita food mess turn today\n",
      "Topic 300: supernatural loveland clock polychronopolous darling bad tropical weed glove automatic\n",
      "Topic 301: cowboys act oooh outta poison andante pray conquers riding billie\n",
      "Topic 302: adore dont cream whiskey aa turnaround rollin middle stinks yak\n",
      "Topic 303: messin daybreak nice oo sickman birthday times mood magic cowboys\n",
      "Topic 304: yak daybreak darling notion katy messin damned cares hello believer\n",
      "Topic 305: excited chiquitita lovelight rooster following hold supernatural replaced begins wont\n",
      "Topic 306: andante notion replaced lucy business supernatural water hollywood paradise movin\n",
      "Topic 307: step andante dreamworld darling dreamin peace oasis imagine headlines tiger\n",
      "Topic 308: rollin dam heels eat steps train black repeat rich andante\n",
      "Topic 309: adore dreamin giddy keepin cecilia grind yonder christ stinks automatic\n",
      "Topic 310: cecilia chiquitita soldiers feels givin confess mask headlines hangin known\n",
      "Topic 311: disillusion chiquitita needs honolulu hawaii going roller chuck cecilia adore\n",
      "Topic 312: daybreak business mercy story strong rubber devil sunny longboat cecilia\n",
      "Topic 313: poison daybreak oasis jesus dreamin birthday american confess runner yonder\n",
      "Topic 314: dont andante adore easy runner notion comes bee bumble friends\n",
      "Topic 315: excited honolulu hawaii believer yonder voodoo conquers gave toys business\n",
      "Topic 316: dam chiquitita believes sunny cecilia confess yonder oo winter price\n",
      "Topic 317: replaced paalam fool brought hide watches destroyed crown wait train\n",
      "Topic 318: dreamworld excited talk bale yonder yak replaced agnetha knowing conquers\n",
      "Topic 319: supernatural nation mexico lot nice andante run shots cecilia gulf\n",
      "Topic 320: tiger stinks paalam bale thank dam afraid desire pink dancing\n",
      "Topic 321: paalam dreamworld hoppin supernatural dreamin replaced act boo yesterdays beneath\n",
      "Topic 322: dreamin poison fever billie paalam destroyed bobby believes problems runner\n",
      "Topic 323: confess afraid mistletoe fashion word ba clock hangin moment supernatural\n",
      "Topic 324: cecilia awhile sickman bother daybreak stinks carolina mask cassandra goodbye\n",
      "Topic 325: conquers believer paalam leaning diamonds darling nation reckless shadow goodbye\n",
      "Topic 326: nation confess party stuff better yesterdays mr crawling laudate notion\n",
      "Topic 327: cecilia rocking billie cassandra notion awhile party oo mask dreams\n",
      "Topic 328: business billie elevator dreamin poison dad heels paki spend needs\n",
      "Topic 329: paalam voodoo messin givin nation cassandra dad fallin dont sore\n",
      "Topic 330: renee voodoo sore strong hough headlines sight roll lived reckless\n",
      "Topic 331: messin confess cryin slippin longboat voodoo true runner outta grind\n",
      "Topic 332: supernatural confess slippin voodoo cryin dont paki lot replaced rise\n",
      "Topic 333: paalam outta rockin sickman ng pneumonia india puso chiquitita fight\n",
      "Topic 334: paalam feels voodoo birthday better cowboys ng jesus yes puso\n",
      "Topic 335: rocket lights birthday habits noise voodoo jesus movin bother going\n",
      "Topic 336: business voodoo supernatural notion beneath confess following doin oasis future\n",
      "Topic 337: stinks longer roller bad chiquitita messin paki dreams tiger throw\n",
      "Topic 338: habits sunshine andante oooh rocking lights faith kisses travelin ba\n",
      "Topic 339: daybreak rooster dog messin miss rubber degree merry sick mr\n",
      "Topic 340: confess times dreamworld money outta natural begins soldiers spirit bobby\n",
      "Topic 341: fever pneumonia runner keepin bit sigh soldiers feet gave lay\n",
      "Topic 342: conquers messin lucy temperature shame cut faith traveler talk livin\n",
      "Topic 343: rocket messin year andante dad hello habits marionette degree doctor\n",
      "Topic 344: stinks awhile tidy mon dreamworld nancy slippin spirit nice grind\n",
      "Topic 345: travelin shame ya cowboys uh leaning carolina kisses voodoo pandora\n",
      "Topic 346: messin supernatural leave habits birthday grace cut dam open reckless\n",
      "Topic 347: bother travelin di games mother ng kong america confess noise\n",
      "Topic 348: voodoo bother dreamin hough roller sealed sail rikky cries dreams\n",
      "Topic 349: business spirits helping conquers shadow midnight daybreak believer easy elevator\n",
      "Topic 350: habits voodoo yonder stinks hello reason roller elevator andante dreamin\n",
      "Topic 351: habits sunshine messin tidy american watches darling shine fallin hear\n",
      "Topic 352: runner rooster tomorrow open boo rodeo bobby mr instead dont\n",
      "Topic 353: degree grind feels middle steps business slippin times messin dad\n",
      "Topic 354: pneumonia dad habits outta runner dam young kiss flu woogie\n",
      "Topic 355: knowing rocket chiquitita elevator haa bad afraid kong business adore\n",
      "Topic 356: business conquers send reckless bale oasis notion carolina oo daybreak\n",
      "Topic 357: elevator nice givin trying hawaii honolulu pain pray seen black\n",
      "Topic 358: stinks america hough sealed business mr daybreak belong fade india\n",
      "Topic 359: di paki cut wag lang ng mong wala sabi puso\n",
      "Topic 360: cassandra aaah business merry hough faith eagle sealed attack mia\n",
      "Topic 361: money cryin sunny raised dam bad looking replaced pain reasons\n",
      "Topic 362: ba dam darling real spend carolina travelin money runner aking\n",
      "Topic 363: hough sealed dreamin tell shot came quarter lovelight hurts lights\n",
      "Topic 364: dreamin ba bobby ways cried longer business rooster seasons happiest\n",
      "Topic 365: hough sealed faith confess helen reckless rollin train cryin sick\n",
      "Topic 366: rooster poison party quarter fat american mistletoe believes fallin woe\n",
      "Topic 367: temperature news soldiers sunshine reckless goes renee shadow jukebox handed\n",
      "Topic 368: mon rocket wasted nancy pneumonia strangers lovers runner hawaii honolulu\n",
      "Topic 369: dam hough sealed andante loaded bridges longboat holy control nothin\n",
      "Topic 370: ba habits hawaii honolulu faith nancy good speak lyin mon\n",
      "Topic 371: habits attack ba daybreak sea magic janie american flowing messin\n",
      "Topic 372: ya doctor que ni spirit jukebox yonder merry crawling maniacs\n",
      "Topic 373: adore reckless darling party watching begins bread devil sealed hough\n",
      "Topic 374: mask hough sealed soldiers broken autumn faith temperature american rocket\n",
      "Topic 375: ba roller oo carolina realized fade dies draw nights rikky\n",
      "Topic 376: sunny turnaround strangers cryin daybreak movin cause que american renee\n",
      "Topic 377: ni awit paalam duyan kong nanay piling cryin runner dam\n",
      "Topic 378: sunshine quarter sentimental wait tennessee jesus rose confess bit watch\n",
      "Topic 379: fallin janie cryin bread rooster fool replaced watches daybreak marry\n",
      "Topic 380: carolina imagine thinking dam carry whistle lot story mama maybe\n",
      "Topic 381: slippin birthday janie evidence sunshine jesus thought business crashing spirit\n",
      "Topic 382: que drown ought lonesome valley boat helen forward jump dixie\n",
      "Topic 383: special degree ni coming someday traveler table awit job delight\n",
      "Topic 384: paalam andante hello ni supernatural temperature hurt pink longer degree\n",
      "Topic 385: habits dad ba uh thinking chains autumn softly fallin lonely\n",
      "Topic 386: merry tomorrow tidings aaah bit sunny coming monsters learned glad\n",
      "Topic 387: fool goes cut carolina burn party died raised cold evidence\n",
      "Topic 388: runner betty beep start waiting lalala crashing dad feels ba\n",
      "Topic 389: helen wishing jukebox faith slippin posses mountain turnaround bless coming\n",
      "Topic 390: mia shot begin messin carolina renee satisfied supernatural miss habits\n",
      "Topic 391: hough sealed fernando mountain bless drown foolish needs awit magic\n",
      "Topic 392: aaah cryin ooooo faith lived knees easy sentimental ought oo\n",
      "Topic 393: send magic closer clouds burnin janie aaah fingers birthday rodeo\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=optimal_num_topics)\n",
    "svd.fit(tfidf_matrix)\n",
    "\n",
    "for topic_idx, topic in enumerate(svd.components_):\n",
    "    top_words_idx = topic.argsort()[::-1][:10]\n",
    "    top_words_for_topic = [words[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {' '.join(top_words_for_topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 0's Top 10 topics: [  9   1  40   7  16   4  33  46 128  27]\n",
      "Song 1's Top 10 topics: [286 242 306 265 195   1 218 169 267 282]\n",
      "Song 2's Top 10 topics: [ 1 40 26 36 13 68 23 45 35 56]\n",
      "Song 3's Top 10 topics: [ 59  54  64  55 177 151 163 143  61 135]\n",
      "Song 4's Top 10 topics: [59 54 64 55 61 40 35 78 39 77]\n"
     ]
    }
   ],
   "source": [
    "topics_for_songs = svd.transform(tfidf_matrix)\n",
    "dom_topics= np.argsort(-topics_for_songs, axis=1)[:, :10]\n",
    "\n",
    "# Print dom topics for first 5 songs\n",
    "for i in range(5):\n",
    "    print(f\"Song {i}'s Top 10 topics: {dom_topics[i] + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match 1: Lonesome Valley Alabama(Similarity: 0.7736665196391537)\n",
      "Match 2: Walk On Down Aerosmith(Similarity: 0.7105783358635683)\n",
      "Match 3: Don't Walk Away Air Supply(Similarity: 0.5881483503502678)\n",
      "Match 4: If You Love Me Air Supply(Similarity: 0.3276703069488694)\n",
      "Match 5: Then Again Alabama(Similarity: 0.31217595573472434)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def find_song_matches_with_svd(user_input, tfidf_vectorizer, svd, topics_for_songs, song_titles, song_artists):\n",
    "    user_tfidf = tfidf_vectorizer.transform([user_input])\n",
    "    user_topics = svd.transform(user_tfidf)\n",
    "\n",
    "    similarities = cosine_similarity(user_topics, topics_for_songs)\n",
    "\n",
    "    top_indices = np.argsort(-similarities[0])[:5]\n",
    "    \n",
    "    # Return matches\n",
    "    return [(song_titles[idx], song_artists[idx], similarities[0, idx]) for idx in top_indices]\n",
    "\n",
    "# Example Usage\n",
    "matches = find_song_matches_with_svd('walk with me my love here in the forests', vectorizer, svd, topics_for_songs, list(lyric_df['song']), list(lyric_df['artist']))\n",
    "\n",
    "for rank, (title, artist, score) in enumerate(matches, start=1):\n",
    "    print(f\"Match {rank}: {title} {artist}(Similarity: {score})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
